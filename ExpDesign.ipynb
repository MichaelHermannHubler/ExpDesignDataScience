{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Expirement Design for Data Science\n",
    "## Group 26\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from os import listdir\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# run in normal python shell once:\n",
    "#>>> import nltk\n",
    "#>>> nltk.download()\n",
    "# and download all packages (easiest)"
   ]
  },
  {
   "source": [
    "### Loading Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 20 newsgroups dataset\n",
    "\n",
    "newsgroups_train = datasets.fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = datasets.fetch_20newsgroups(subset='test')\n",
    "\n",
    "# Subjectivity dataset\n",
    "subjective_sentences_file = './data/Movie_Review/Subjectivity/plot.tok.gt9.5000'\n",
    "objective_sentences_file = './data/Movie_Review/Subjectivity/quote.tok.gt9.5000'\n",
    "\n",
    "subjective_sentences = pd.read_csv(subjective_sentences_file, header=None, sep='|', encoding='latin-1', names=['text'])\n",
    "objective_sentences = pd.read_csv(objective_sentences_file, header=None, sep='|', encoding='latin-1', names=['text'])\n",
    "\n",
    "subjective_sentences['subjective'] = 1\n",
    "objective_sentences['subjective'] = 0\n",
    "\n",
    "subjectivity_complete = pd.concat([subjective_sentences, objective_sentences])\n",
    "\n",
    "# Sentence polarity dataset\n",
    "\n",
    "neg_directory = './data/Movie_Review/Sentence_Polarity/neg/'\n",
    "pos_directory = './data/Movie_Review/Sentence_Polarity/pos/'\n",
    "\n",
    "neg_files = os.listdir(neg_directory)\n",
    "pos_files = os.listdir(pos_directory)\n",
    "\n",
    "neg_sentences = []\n",
    "pos_sentences = []\n",
    "\n",
    "for file in neg_files:\n",
    "    with open(neg_directory + file, 'r') as f:\n",
    "        neg_sentences.append(f.read())\n",
    "        \n",
    "for file in pos_files:\n",
    "    with open(pos_directory + file, 'r') as f:\n",
    "        pos_sentences.append(f.read())\n",
    "\n",
    "neg_sentences = pd.DataFrame(neg_sentences, columns=['text'])\n",
    "pos_sentences = pd.DataFrame(pos_sentences, columns=['text'])\n",
    "\n",
    "neg_sentences['positive'] = 0\n",
    "pos_sentences['positive'] = 1\n",
    "\n",
    "polarity_complete = pd.concat([neg_sentences, pos_sentences])\n",
    "\n",
    "# TREC 45\n",
    "\n",
    "### TODO\n",
    "\n",
    "# Debug\n",
    "\n",
    "#print(newsgroups_train)\n",
    "#print(subjectivity_complete.head())\n",
    "#print(polarity_complete.head())"
   ]
  },
  {
   "source": [
    "### Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "def tokenize_text (text):\n",
    "    tokens = []\n",
    "    for sentences in nltk.sent_tokenize(text):\n",
    "        for token in nltk.word_tokenize(text):\n",
    "            if token not in stopwords.words('english'):\n",
    "                tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "# Tokenize Subjectivity\n",
    "subjectivity_tokenized = [tokenize_text(text) for text in subjectivity_complete['text']]   # Tokenized data\n",
    "subjectivity_complete['tokens'] = subjectivity_tokenized\n",
    "\n",
    "# Tokenize Polarity\n",
    "polarity_tokenized = [tokenize_text(text) for text in polarity_complete['text']]   # Tokenized data\n",
    "polarity_complete['tokens'] = polarity_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}